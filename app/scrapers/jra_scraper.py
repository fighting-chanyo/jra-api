import os
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from app.schemas import IpatAuth
from app.services.parsers import parse_jra_csv
from app.constants import BET_TYPE_MAP

def scrape_past_history_csv(creds: IpatAuth):
    """Playwright„Å´„Çà„Çã„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Å®CSV„Éë„Éº„ÇπÂá¶ÁêÜ„ÇíÊãÖ„ÅÜ (Êóßsync_past_history)"""
    print("üöÄ Accessing JRA Vote Inquiry (PC/CSV Mode)...")
    all_parsed_data = []
    
    with sync_playwright() as p:
        is_headless = os.getenv("HEADLESS", "true").lower() != "false"
        browser = p.chromium.launch(
            headless=is_headless,
            args=["--disable-cache", "--disk-cache-size=0"]
        )
        # User-Agent„ÇíË®≠ÂÆö„Åó„Å¶„ÄÅ‰∏ÄËà¨ÁöÑ„Å™„Éñ„É©„Ç¶„Ç∂„Åã„Çâ„ÅÆ„Ç¢„ÇØ„Çª„Çπ„Å´Ë¶ã„Åõ„Åã„Åë„Çã
        context = browser.new_context(
            accept_downloads=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        context.route("**/*.{png,jpg,jpeg,gif,webp,svg,woff,woff2,css}", lambda route: route.abort())
        page = context.new_page()
        page.on("dialog", lambda dialog: dialog.accept())
        
        print("üëâ Logging in to PC site...")
        page.goto("https://www.nvinq.jra.go.jp/jra/")
        
        # „Éá„Éê„ÉÉ„Ç∞Áî®Ôºö„É≠„Ç∞„Ç§„É≥„Éö„Éº„Ç∏‰øùÂ≠ò
        with open("debug_login_page.html", "w", encoding="utf-8") as f: f.write(page.content())

        # ÂÖ•ÂäõÂÄ§„Çí„ÇØ„É™„Éº„Éã„É≥„Ç∞ÔºàÂâçÂæå„ÅÆÁ©∫ÁôΩÂâäÈô§Ôºâ
        s_no = creds.subscriber_number.strip()
        pwd = creds.password.strip()
        pars = creds.pars_number.strip()

        page.wait_for_selector("#UID")
        page.locator("#UID").fill(s_no)
        page.wait_for_timeout(500)
        # „É¶„Éº„Ç∂„ÉºÊåáÊëò„Å´„Çà„ÇäÂÖ•„ÇåÊõø„Åà: ÊöóË®ºÁï™Âè∑Ê¨Ñ„Å´P-ARS„ÄÅP-ARSÊ¨Ñ„Å´ÊöóË®ºÁï™Âè∑„ÇíÂÖ•Âäõ
        page.locator("#PWD").fill(pars)
        page.wait_for_timeout(500)
        page.locator("#PARS").fill(pwd)
        page.wait_for_timeout(500)
        page.locator("input[type='submit'][value='„É≠„Ç∞„Ç§„É≥']").click()
        page.wait_for_load_state("networkidle")

        # „Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏„ÉÅ„Çß„ÉÉ„ÇØ
        if page.locator("text=Âä†ÂÖ•ËÄÖÁï™Âè∑„ÉªÊöóË®ºÁï™Âè∑„ÉªP-ARSÁï™Âè∑„Å´Ë™§„Çä„Åå„ÅÇ„Çä„Åæ„Åô").is_visible():
             with open("debug_login_failed.html", "w", encoding="utf-8") as f: f.write(page.content())
             raise Exception("Login Failed: Invalid Credentials (Âä†ÂÖ•ËÄÖÁï™Âè∑„ÉªÊöóË®ºÁï™Âè∑„ÉªP-ARSÁï™Âè∑„Å´Ë™§„Çä„Åå„ÅÇ„Çä„Åæ„Åô)")

        print("üëâ Navigating to Vote Inquiry (JRAWeb320)...")
        menu_btn = page.locator("tr:has-text('ÊäïÁ•®ÂÜÖÂÆπÁÖß‰ºö') input[type='submit']").first
        if not menu_btn.is_visible():
            menu_btn = page.locator("input[value='ÈÅ∏Êäû']").first
        if not menu_btn.is_visible():
            with open("debug_login_failed.html", "w", encoding="utf-8") as f: f.write(page.content())
            raise Exception("Login Failed or Menu Changed. See debug_login_failed.html")
        menu_btn.click()
        page.wait_for_load_state("networkidle")

        print("üëâ Navigating to Receipt Number List (JRAWeb020)...")
        accept_link = page.locator("a.toAcceptnoNum")
        if accept_link.is_visible():
            accept_link.click()
        else:
            page.evaluate("document.forms['Go020'].submit()")
        
        # --- „Åì„Åì„Åã„Çâ‰øÆÊ≠£ ---
        # „Éö„Éº„Ç∏ÈÅ∑Áßª„ÇíÂæÖÊ©ü„Åó„ÄÅ„Åæ„Åö„Çª„ÉÉ„Ç∑„Éß„É≥„Ç®„É©„Éº„ÅÆÂèØËÉΩÊÄß„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åô„Çã
        try:
            # „ÄåÊó•‰ªòÈÅ∏Êäû„Äç„Éö„Éº„Ç∏„ÄÅ„Åæ„Åü„ÅØ„Äå„Çª„ÉÉ„Ç∑„Éß„É≥Âàá„Çå„Äç„Éö„Éº„Ç∏„ÅÆ„Å©„Å°„Çâ„Åã„ÅÆË™≠„ÅøËæº„Åø„ÅåÂÆå‰∫Ü„Åô„Çã„ÅÆ„ÇíÂæÖ„Å§
            page.wait_for_load_state("domcontentloaded", timeout=15000)

            # „Çª„ÉÉ„Ç∑„Éß„É≥Âàá„ÇåÁîªÈù¢„ÅÆÁâπÊúâ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅåÂ≠òÂú®„Åô„Çã„Åã„Å©„ÅÜ„Åã„ÅßÂà§ÂÆö
            if page.locator("text=„É≠„Ç∞„Ç§„É≥„ÅåÁÑ°Âäπ„Å®„Å™„Å£„Åü„Åã").is_visible():
                raise Exception("Session timed out or became invalid. Please try again.")

        except Exception as e:
            # is_visible()„ÅÆ„Çø„Ç§„É†„Ç¢„Ç¶„Éà„ÇÑ„ÄÅÁã¨Ëá™„Å´raise„Åó„Åü‰æãÂ§ñ„ÇíÊçïÊçâ
            error_message = str(e) if str(e) else "Failed to determine page state after navigation."
            # „Éá„Éê„ÉÉ„Ç∞Áî®„Å´ÊúÄÁµÇÁöÑ„Å™ÁîªÈù¢„Çí‰øùÂ≠ò
            with open("debug_navigation_error.html", "w", encoding="utf-8") as f:
                f.write(page.content())
            raise Exception(error_message)

        print("üëâ Checking Date List...")
        date_buttons = page.locator("input[value='ÈÅ∏Êäû']")
        date_count = date_buttons.count()
        print(f"üëÄ Found {date_count} date buttons.")

        if date_count == 0:
            # „Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ„ÅØÈÄöÈÅé„Åó„Åü„Åå„Éú„Çø„É≥„Åå„Å™„ÅÑÂ†¥Âêà
            print("‚ö†Ô∏è No dates found. Maybe no betting history.")
            return []
        # --- „Åì„Åì„Åæ„Åß‰øÆÊ≠£ ---

        for i in range(date_count):
            page.locator("input[value='ÈÅ∏Êäû']").nth(i).click()
            page.wait_for_load_state("networkidle")
            csv_btn = page.locator("form[action*='JRACSVDownload'] input[name='normal']")
            if csv_btn.is_visible():
                with page.expect_download() as download_info:
                    csv_btn.click()
                download = download_info.value
                csv_path = f"temp_history_{i}.csv"
                download.save_as(csv_path)
                parsed = parse_jra_csv(csv_path)
                all_parsed_data.extend(parsed)
                os.remove(csv_path)
            
            back_btn = page.locator("input[value*='Êó•‰ªòÈÅ∏Êäû']").first
            if back_btn.is_visible():
                back_btn.click()
            else:
                page.go_back()
            page.wait_for_load_state("networkidle")

        browser.close()

    return all_parsed_data

def _parse_recent_detail_html(html_content, receipt_no, date_str):
    """Áõ¥ËøëÊäïÁ•®Â±•Ê≠¥Ë©≥Á¥∞HTML„Çí„Éë„Éº„Çπ„Åô„Çã"""
    soup = BeautifulSoup(html_content, "html.parser")
    parsed_tickets = []
    
    rows = soup.select("table.table-result tbody tr")
    line_counter = 1
    
    for row in rows:
        # Skip header/footer rows or empty rows
        classes = row.get("class", [])
        if "list-footer" in classes or "print-only" in classes:
            continue
            
        # Check if it's a data row (has td.race-info)
        race_info_td = row.select_one("td.race-info")
        if not race_info_td:
            continue
            
        text_content = race_info_td.get_text(separator=" ", strip=True)
        
        # Parse Place
        # "‰∏≠‰∫¨ ÔºàÂúüÔºâ 8R" -> "‰∏≠‰∫¨"
        place_match = re.search(r"^([^\s]+)", text_content)
        race_place = place_match.group(1) if place_match else "Unknown"
        
        # Parse Weekday and Calculate Date
        # "‰∏≠‰∫¨ ÔºàÂúüÔºâ 8R" -> "Âúü"
        weekday_match = re.search(r"Ôºà(.)Ôºâ", text_content)
        race_weekday_str = weekday_match.group(1) if weekday_match else None
        
        calculated_date_str = date_str
        if race_weekday_str:
            try:
                # ÁØÄ„ÅÆ„Ç¢„É≥„Ç´„ÉºÊó•ÔºàÂúüÊõúÊó•Ôºâ„ÇíÂü∫Ê∫ñ„Å´Êó•‰ªò„ÇíÊ±∫ÂÆö„Åô„Çã„É≠„Ç∏„ÉÉ„ÇØ
                # 1. „Çπ„ÇØ„É¨„Ç§„ÉóÂÆüË°åÊó•(D)„ÇíÂü∫Ê∫ñ„Å´„Åô„Çã
                scrape_date = datetime.now()
                scrape_weekday = scrape_date.weekday() # Mon=0, ..., Sun=6
                
                # 2. D„Å´ÊúÄ„ÇÇËøë„ÅÑÂúüÊõúÊó•(S)„ÇíÊ±Ç„ÇÅ„Çã
                # ÂúüÊõú=5. offset = (5 - scrape_weekday + 3) % 7 - 3
                # Êúà(0) -> -2 (Ââç„ÅÆÂúüÊõú), Èáë(4) -> +1 (Ê¨°„ÅÆÂúüÊõú), Âúü(5) -> 0, Êó•(6) -> -1 (Ââç„ÅÆÂúüÊõú)
                offset_to_saturday = (5 - scrape_weekday + 3) % 7 - 3
                anchor_saturday = scrape_date + timedelta(days=offset_to_saturday)
                
                # 3. „É¨„Éº„ÇπÈñãÂÇ¨Êó•„ÅÆ„Ç™„Éï„Çª„ÉÉ„Éà„ÇíË®àÁÆó
                # Èáë: -1, Âúü: 0, Êó•: 1, Êúà: 2, ÁÅ´: 3
                target_weekday_map = {'Èáë': -1, 'Âúü': 0, 'Êó•': 1, 'Êúà': 2, 'ÁÅ´': 3}
                
                if race_weekday_str in target_weekday_map:
                    day_diff = target_weekday_map[race_weekday_str]
                    race_date = anchor_saturday + timedelta(days=day_diff)
                    calculated_date_str = race_date.strftime("%Y%m%d")
                else:
                    # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂæìÊù•„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞ÔºàÊ∞¥„ÉªÊú®„Å™„Å©Ôºâ
                    # Âü∫Êú¨ÁöÑ„Å´„ÅÇ„ÇäÂæó„Å™„ÅÑ„Åå„ÄÅÂøµ„ÅÆ„Åü„ÇÅÂΩìÊó•„ÅãÊú™Êù•„ÅÆÁõ¥Ëøë„ÅÆÊó•‰ªò„Å®„Åô„Çã
                    weekday_map = {'Ê∞¥': 2, 'Êú®': 3}
                    if race_weekday_str in weekday_map:
                        base_weekday = scrape_weekday
                        target_weekday = weekday_map[race_weekday_str]
                        diff_days = (target_weekday - base_weekday + 7) % 7
                        race_date = scrape_date + timedelta(days=diff_days)
                        calculated_date_str = race_date.strftime("%Y%m%d")
            except Exception as e:
                print(f"‚ö†Ô∏è Date calculation failed: {e}")

        # Parse Race No
        race_no_match = re.search(r"(\d+)R", text_content)
        race_number_str = race_no_match.group(1) if race_no_match else "00"
        
        # Parse Bet Type
        # Try to find specific span for bet type first
        bet_type_span = race_info_td.select_one("span.space-2")
        bet_type_raw = bet_type_span.get_text(strip=True) if bet_type_span else "Unknown"
        
        # Map to English code
        bet_type_code = "unknown"
        for jp, en in BET_TYPE_MAP.items():
            if jp in bet_type_raw:
                bet_type_code = en
                break
        
        # Parse Buy Type (Method)
        buy_type_raw = "ÈÄöÂ∏∏"
        # The buy type is usually in the span after bet type, or just text.
        element_blocks = race_info_td.select("span.element-block")
        if len(element_blocks) >= 3:
             buy_type_raw = element_blocks[-1].get_text(strip=True)
        elif "ÊµÅ„Åó" in text_content or "„Å™„Åå„Åó" in text_content: buy_type_raw = "„Å™„Åå„Åó"
        elif "„Éú„ÉÉ„ÇØ„Çπ" in text_content: buy_type_raw = "„Éú„ÉÉ„ÇØ„Çπ"
        elif "„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥" in text_content: buy_type_raw = "„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥"

        method = "NORMAL"
        multi = False
        if "Ôº¢ÔºØÔº∏" in buy_type_raw or "„Éú„ÉÉ„ÇØ„Çπ" in buy_type_raw:
            method = "BOX"
        elif "„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥" in buy_type_raw:
            method = "FORMATION"
        elif "„Å™„Åå„Åó" in buy_type_raw or "ÊµÅ„Åó" in buy_type_raw:
            method = "NAGASHI"
            if "„Éû„É´„ÉÅ" in buy_type_raw:
                multi = True
        
        # Content Parsing using .print-only
        axis = []
        partners = []
        selections = []
        positions = []
        
        horse_combi_td = row.select_one("td.horse-combi")
        print_only_div = horse_combi_td.select_one(".print-only")
        
        parsed_from_print_only = False

        if print_only_div:
            # Try to parse from .print-only section (for complex bets)
            flex_rows = print_only_div.select(".flex")
            if flex_rows:
                parsed_from_print_only = True
                if method == "NAGASHI":
                    for flex in flex_rows:
                        prefix = flex.select_one(".method-prefix")
                        val_div = flex.select_one(".ng-binding")
                        if prefix and val_div:
                            p_text = prefix.get_text(strip=True)
                            v_text = val_div.get_text(strip=True)
                            nums = [x.strip() for x in v_text.replace(" ", "").split(",") if x.strip()]
                            
                            if "Áõ∏Êâã" in p_text:
                                partners.extend(nums)
                            else:
                                # "1ÁùÄ:", "Ëª∏:", "1È†≠ÁõÆ:" etc. treat as axis
                                
                                # Parse position from prefix for Fixed Nagashi
                                current_positions = []
                                if not multi:
                                    # Regex to find 1-3 (half or full width) followed by ÁùÄ or È†≠ÁõÆ
                                    pos_match = re.search(r"([123ÔºëÔºíÔºì„Éª]+)(?:ÁùÄ|È†≠ÁõÆ)", p_text)
                                    if pos_match:
                                        pos_str = pos_match.group(1)
                                        pos_map = {"1": 1, "2": 2, "3": 3, "Ôºë": 1, "Ôºí": 2, "Ôºì": 3}
                                        for char in pos_str:
                                            if char in pos_map:
                                                current_positions.append(pos_map[char])
                                
                                if not current_positions:
                                    axis.extend(nums)
                                else:
                                    # If positions found, add axis and positions for each
                                    for pos in current_positions:
                                        axis.extend(nums)
                                        positions.extend([pos] * len(nums))
                elif method == "FORMATION":
                    # Formation usually lists selections for each position
                    for flex in flex_rows:
                        val_div = flex.select_one(".ng-binding")
                        if val_div:
                            v_text = val_div.get_text(strip=True)
                            nums = [x.strip() for x in v_text.replace(" ", "").split(",") if x.strip()]
                            selections.append(nums)
                else:
                    # Fallback for other types if they appear in flex
                    pass
            
            # If not flex, maybe just text (e.g. BOX)
            if not parsed_from_print_only:
                text = print_only_div.get_text(strip=True)
                if text:
                    # Simple extraction of numbers
                    nums = re.findall(r"\d+", text)
                    if nums:
                        parsed_from_print_only = True
                        if method == "BOX":
                            selections.append(nums)
                        elif method == "NORMAL":
                            selections.append(nums)

        # Fallback if print-only didn't yield results (or for Normal bets where print-only might be empty)
        if not parsed_from_print_only:
             # Use horse-combi-list
             combi_spans = horse_combi_td.select("span.set-heading")
             if combi_spans:
                 nums = [s.get_text(strip=True) for s in combi_spans]
                 selections.append(nums)
             else:
                 # Text fallback
                 text = horse_combi_td.get_text(strip=True)
                 nums = re.findall(r"\d+", text)
                 if nums:
                    selections.append(nums)

        # Extract Amount per point
        money_td = row.select_one("td.money")
        amount_per_point = 0
        if money_td:
            # The first div.ng-binding contains the "per point" amount (e.g., "200ÂÜÜ")
            money_div = money_td.select_one("div.ng-binding")
            if money_div:
                amount_text = money_div.get_text(strip=True).replace("ÂÜÜ", "").replace(",", "")
                try:
                    amount_per_point = int(amount_text)
                except:
                    amount_per_point = 0
        
        if amount_per_point == 0:
            amount_per_point = 100 # Default fallback

        # Sets (points)
        sets_td = row.select_one("td.sets")
        total_points = 0
        if sets_td:
            sets_text = sets_td.get_text(strip=True).replace("ÁµÑ", "").replace(",", "")
            try:
                total_points = int(sets_text)
            except:
                total_points = 0
        
        # If points couldn't be parsed, assume at least 1
        if total_points == 0:
            total_points = 1

        # Calculate total_cost strictly as (amount_per_point * total_points)
        calculated_total_cost = amount_per_point * total_points

        payout = 0
        status = "PENDING"
        
        ticket = {
            "raw": {
                "race_date_str": calculated_date_str,
                "race_place": race_place,
                "race_number_str": race_number_str,
                "receipt_no": receipt_no,
                "line_no": line_counter
            },
            "parsed": {
                "bet_type": bet_type_code,
                "buy_type": method,
                "content": {
                    "type": bet_type_code,
                    "method": method,
                    "multi": multi,
                    "axis": axis,
                    "partners": partners,
                    "selections": selections,
                    "positions": positions
                },
                "amount_per_point": amount_per_point,
                "total_points": total_points,
                "total_cost": calculated_total_cost,
                "payout": payout,
                "status": status,
                "source": "IPAT_RECENT",
                "mode": "REAL"
            }
        }
        
        parsed_tickets.append(ticket)
        line_counter += 1
        
    return parsed_tickets

def scrape_recent_history(creds: IpatAuth):
    """Playwright„Å´„Çà„Çã„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞ (Recent History Mode)"""
    print("üöÄ Accessing JRA IPAT (Recent History Mode)...")
    all_parsed_data = []
    
    with sync_playwright() as p:
        is_headless = os.getenv("HEADLESS", "true").lower() != "false"
        browser = p.chromium.launch(
            headless=is_headless,
            args=["--disable-cache", "--disk-cache-size=0"]
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        # Step 1: Top Page (INET-ID)
        print("üëâ Logging in to IPAT (Step 1: INET-ID)...")
        page.goto("https://www.ipat.jra.go.jp/")
        
        # Check for closed message
        if page.locator("text=„Åü„Å†„ÅÑ„Åæ„ÅÆÊôÇÈñì„ÅØÊäïÁ•®Âèó‰ªòÊôÇÈñìÂ§ñ„Åß„Åô„ÄÇ").is_visible():
             raise Exception("JRA IPAT is currently closed.")
        
        inet_id = creds.inet_id.strip()
        if not inet_id:
             raise Exception("INET-ID is missing")

        page.fill("input[name='inetid']", inet_id)
        with page.expect_navigation(wait_until="domcontentloaded"):
            page.click("p.button a[title='„É≠„Ç∞„Ç§„É≥']")
            
        # Step 2: Subscriber Info
        print("üëâ Logging in to IPAT (Step 2: Subscriber Info)...")
        page.wait_for_selector("input[name='i']")
        page.fill("input[name='i']", creds.subscriber_number.strip())
        page.fill("input[name='p']", creds.password.strip())
        page.fill("input[name='r']", creds.pars_number.strip())
        with page.expect_navigation(wait_until="domcontentloaded"):
            page.click("a[title='„Éç„ÉÉ„ÉàÊäïÁ•®„É°„Éã„É•„Éº„Å∏']")

        # Step 3: Go to Vote History (Recent)
        print("üëâ Logging in to IPAT (Step 3: Vote History)...")
        page.wait_for_load_state("networkidle")
        history_btn_selector = "button.btn-reference"
        page.wait_for_selector(history_btn_selector)
        page.click(history_btn_selector)
        page.wait_for_selector("h1:has-text('ÊäïÁ•®Â±•Ê≠¥‰∏ÄË¶ß')")

        # Step 4: Iterate through Today and Yesterday
        print("üëâ Checking for history items (Today & Yesterday)...")
        
        target_days = [
            ("Today", "label[for='refer-today']", 0),
            ("Yesterday", "label[for='refer-before']", 1)
        ]
        
        for day_name, selector, day_offset in target_days:
            print(f"üëâ Switching to {day_name}...")
            page.click(selector)
            
            try:
                page.wait_for_selector("div.list-loading", state="visible", timeout=1000)
                page.wait_for_selector("div.list-loading", state="hidden", timeout=10000)
            except:
                page.wait_for_timeout(1000)
            
            # Calculate Date String (YYYYMMDD)
            target_date = datetime.now() - timedelta(days=day_offset)
            date_str = target_date.strftime("%Y%m%d")

            # Get number of receipts
            rows = page.locator("table.table-status tbody tr")
            count = rows.count()
            print(f"üëÄ Found {count} history items for {day_name}.")
            
            for i in range(count):
                # Re-query rows to avoid stale elements
                rows = page.locator("table.table-status tbody tr")
                row = rows.nth(i)
                
                # Check for "No History" message
                try:
                    row_text = row.inner_text()
                    if "ÊäïÁ•®Â±•Ê≠¥„Åå„ÅÇ„Çä„Åæ„Åõ„Çì" in row_text:
                        print(f"   ‚ÑπÔ∏è No history found for {day_name}. Skipping.")
                        break
                except:
                    pass

                # Extract Receipt No
                try:
                    receipt_no = row.locator("td.receipt a").inner_text().strip()
                except:
                    print(f"‚ö†Ô∏è Could not extract receipt no for row {i}")
                    # Only print HTML if it's not the "No history" row (which we should have caught above, but just in case)
                    if "ÊäïÁ•®Â±•Ê≠¥„Åå„ÅÇ„Çä„Åæ„Åõ„Çì" not in row.inner_html():
                        print(f"   Row HTML: {row.inner_html()}")
                    continue

                print(f"   Processing Receipt: {receipt_no}")
                
                # Click to open details
                try:
                    target_link = row.locator("td.receipt a")
                    target_link.scroll_into_view_if_needed()
                    target_link.click()
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Failed to click receipt {receipt_no}: {e}")
                    continue

                # Wait for detail view
                is_detail_loaded = False
                try:
                    # Wait for either the header or a known element in the detail view
                    page.wait_for_selector("h1:has-text('ÊäïÁ•®Â±•Ê≠¥ÁµêÊûúÂÜÖÂÆπ'), table.table-result", timeout=10000)
                    is_detail_loaded = True
                except:
                    print(f"   ‚ö†Ô∏è Failed to load detail view for {receipt_no}. Attempting to recover...")
                
                if is_detail_loaded:
                    # Parse Detail HTML
                    content = page.content()
                    try:
                        parsed = _parse_recent_detail_html(content, receipt_no, date_str)
                        all_parsed_data.extend(parsed)
                        print(f"   ‚úÖ Extracted {len(parsed)} tickets.")
                    except Exception as e:
                        print(f"   ‚ùå Error parsing detail for {receipt_no}: {e}")
                
                # Go back to list (Always try to go back if we might have moved)
                try:
                    # Try "‰∏ÄË¶ß„Å´Êàª„Çã" button
                    back_btn = page.locator("button:has-text('‰∏ÄË¶ß„Å´Êàª„Çã')")
                    if back_btn.is_visible():
                        back_btn.click()
                    else:
                        # Fallback: Close detail modal if it's a modal
                        close_btn = page.locator("button:has-text('Èñâ„Åò„Çã')").last
                        if close_btn.is_visible():
                            close_btn.click()
                    
                    # Wait for list to reappear
                    page.wait_for_selector("h1:has-text('ÊäïÁ•®Â±•Ê≠¥‰∏ÄË¶ß')", timeout=5000)
                except:
                    # If we can't find the back button or list header, we might already be on the list or stuck
                    if "ÊäïÁ•®Â±•Ê≠¥‰∏ÄË¶ß" not in page.content():
                        print(f"   ‚ö†Ô∏è Could not confirm return to list for {receipt_no}. Reloading page...")
                        page.reload()
                        page.wait_for_selector("h1:has-text('ÊäïÁ•®Â±•Ê≠¥‰∏ÄË¶ß')")
                
        browser.close()
        
    return all_parsed_data
